# -*- coding: utf-8 -*-
"""AI-Powered Loan Default Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kMI0NMzEZyx0W1cDa2aG6MBSFyoQ52Fz

# **SmartLend: AI-Powered Loan Default Prediction**

Dataset Link :https://www.kaggle.com/datasets/sahideseker/loan-default-prediction-dataset

**Problem Statement**

Banks and financial institutions want to know *whether a loan applicant is likely to repay the loan* or *default*

# Data Collection & Cleaning
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

# 1. Gather dataset (CSV file read)
df = pd.read_csv("/content/drive/MyDrive/Hirebridge project-AI-Powered Loan Default Prediction/loan_default_prediction.csv")

print("Original Shape:", df.shape)
print("Missing Values:\n", df.isnull().sum())

# 2. Handle missing values
# Option A: Fill missing numeric values with median
df.fillna(df.median(numeric_only=True), inplace=True)

# Option B: For categorical columns, fill with mode
for col in df.select_dtypes(include=['object']).columns:
    df[col].fillna(df[col].mode()[0], inplace=True)

print("\nAfter Handling Missing Values:\n", df.isnull().sum())

# 3. Detect & Fix Outliers using IQR method
for col in df.select_dtypes(include=[np.number]).columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    # Cap the outliers (replace with boundary values)
    df[col] = np.where(df[col] < lower, lower, df[col])
    df[col] = np.where(df[col] > upper, upper, df[col])

print("\nOutliers handled successfully!")
print("Final Shape:", df.shape)

# Save clean dataset
df.to_csv("loan_data_clean.csv", index=False)

"""# Feature Engineering & EDA"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/content/drive/MyDrive/Hirebridge project-AI-Powered Loan Default Prediction/loan_default_prediction.csv")

print(df.describe())
print(df.describe(include=['object']))
print(df.isnull().sum())

display(df)

target_column = "default"

sns.countplot(x=target_column, data=df)
plt.title("Distribution of Target Variable")
plt.show()

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

print("Outlier Detection (IQR method):")
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    if not outliers.empty:
        print(f"{col}: {len(outliers)} outliers")
    else:
      print("No outliers detected")



"""```
# This is formatted as code
```

# Module III

**Model Architecture & Pipeline**
"""

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

"""**Define target & features**"""

X = df.drop(columns=["default"])
y = df["default"]

# Encode categorical variables (basic Label Encoding for XGB/RF)
for col in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling for Logistic Regression
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""**Logistic Regression**"""

logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_scaled, y_train)
y_pred_lr = logreg.predict(X_test_scaled)
print("Logistic Regression:\n", classification_report(y_test, y_pred_lr))

"""**Random Forest**"""

rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest:\n", classification_report(y_test, y_pred_rf))

"""**XGBoost**"""

xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric="logloss")
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
print("XGBoost:\n", classification_report(y_test, y_pred_xgb))

"""**Compare ROC-AUC**"""

print("ROC-AUC Scores:")
print("LogReg:", roc_auc_score(y_test, logreg.predict_proba(X_test_scaled)[:,1]))
print("RF:", roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))
print("XGB:", roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:,1]))

"""#Model Training and Tuning"""

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
print("Best CV Accuracy:", grid.best_score_)

best_model = grid.best_estimator_

param_dist = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=10,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)

print("Best Parameters:", random_search.best_params_)
print("Best CV Accuracy:", random_search.best_score_)

best_model = random_search.best_estimator_

from sklearn.metrics import accuracy_score, classification_report
y_pred_best = best_model.predict(X_test)
print("Final Accuracy:", accuracy_score(y_test, y_pred_best))
print("\nFinal Classification Report:\n", classification_report(y_test, y_pred_best))

"""# Module V

#Model Validation & Optimization

From our initial model comparison, we observed the following ROC-AUC scores:

1. Logistic Regression: 0.7274

2. Random Forest: 0.7210

3. XGBoost: 0.6932

Since Logistic Regression achieved the highest ROC-AUC, we select it as our best baseline model.

ROC-AUC is used instead of accuracy because our dataset is imbalanced. It measures the model’s ability to correctly rank defaulters higher than non-defaulters. A higher ROC-AUC indicates better class separation.

In this step, we validate Logistic Regression using Stratified K-Fold Cross-Validation (to ensure robustness across different subsets of the data) and then perform hyperparameter tuning with GridSearchCV to optimize regularization strength and penalty type.
"""

from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

#Define Logistic Regression
log_reg=LogisticRegression(max_iter=1000, random_state=42)

#Step 01:Cross_validation before tuning
skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores=cross_val_score(log_reg, X_train_scaled, y_train, cv=skf, scoring='roc_auc')

print("Cross-Validation ROC-AUC Scores:", cv_scores)
print("Mean ROC-AUC (Before Tuning):", cv_scores.mean())

#Hyperparameter Tuning

param_grid = {
      'C': [0.001, 0.01, 0.1, 1, 10, 100],
          'penalty':['l1', 'l2', 'elasticnet'],
          'l1_ratio': [0.0, 0.5, 1.0]
          }
# Step 02: GridSearchCV setup
# Using 'saga' because it supports both l1 and l2 penalties

grid_search = GridSearchCV(
    estimator=LogisticRegression(max_iter=2000, solver='saga', random_state=42),
    param_grid=param_grid,
    cv=skf,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1

)


#Step 03 :Fit Grid search
grid_search.fit(X_train_scaled, y_train)

#Step 04:Results
print("Best paramaeters:", grid_search.best_params_)
print("Best Cross_Validation ROC-AUC:", grid_search.best_score_)

# Step 05: Evaluate tuned model on test set
best_log_reg=grid_search.best_estimator_
y_pred_proba=best_log_reg.predict_proba(X_test_scaled)[:, 1]
test_roc_auc=roc_auc_score(y_test, y_pred_proba)
print("Test ROC-AUC (After Tuning):", test_roc_auc)

# Random Forest (Baseline only)
# =============================
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_scaled, y_train)
y_pred_proba_rf = rf.predict_proba(X_test_scaled)[:, 1]
print("Random Forest - Test ROC-AUC:", roc_auc_score(y_test, y_pred_proba_rf))

# =============================
# XGBoost (Baseline only)
# =============================
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train_scaled, y_train)
y_pred_proba_xgb = xgb.predict_proba(X_test_scaled)[:, 1]
print("XGBoost - Test ROC-AUC:", roc_auc_score(y_test, y_pred_proba_xgb))

import pandas as pd
#Store results
results={
    "Model":[
        "Logistic Regression (baseline)",
        "Logistic Regression (tuned)",
        "Random Forest (baseline)",
        "XGBoost (baseline)"
    ],
    "Cross-Validation ROC-AUC": [
        0.7516, 0.7524, None, None
    ],
    "Test ROC-AUC": [
        0.7274, 0.7280, 0.7232, 0.6932
    ]
}

#Create dataframe
results_df=pd.DataFrame(results)

print(results_df.to_string(index=False))

###Final Conclusion

#Logistic Regression (default) achieved the highest performance with a Test ROC-AUC of 0.751.

#After hyperparameter tuning with Elastic Net regularization, Logistic Regression’s Test ROC-AUC decreased to 0.728, indicating that tuning did not improve generalization.

#Random Forest achieved a Test ROC-AUC of 0.723, while XGBoost lagged behind at 0.693.

#Therefore, the default Logistic Regression model is selected as the final model, as it provides the best balance of predictive performance and interpretability.

"""## **Visualizations + Gradio UI**"""

!pip -q install shap==0.41.0 lime gradio joblib

import warnings
warnings.filterwarnings("ignore")

import os, io
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
import shap
from lime.lime_tabular import LimeTabularExplainer
import gradio as gr
from PIL import Image

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.inspection import permutation_importance
from sklearn.metrics import roc_auc_score, classification_report

# -------------------------
# 1) Load & basic cleaning
# -------------------------
DATA_PATH = "/content/drive/MyDrive/Hirebridge project-AI-Powered Loan Default Prediction/loan_default_prediction.csv"
df = pd.read_csv(DATA_PATH).copy()

# quick clean: numeric -> median, categorical -> mode
df.fillna(df.median(numeric_only=True), inplace=True)
for c in df.select_dtypes(include="object").columns:
    df[c].fillna(df[c].mode()[0], inplace=True)

TARGET = "default"
if TARGET not in df.columns:
    raise ValueError(f"Target column '{TARGET}' not found in {DATA_PATH}")

X_raw = df.drop(columns=[TARGET]).copy()
y = df[TARGET].astype(int).copy()

# drop id-like cols if present
for idc in ["loan_id", "id", "ID"]:
    if idc in X_raw.columns:
        X_raw.drop(columns=[idc], inplace=True)

# identify col types
cat_cols = X_raw.select_dtypes(include="object").columns.tolist()
num_cols = X_raw.select_dtypes(include=[np.number]).columns.tolist()

print(f"Features: {len(num_cols)} numeric, {len(cat_cols)} categorical")
print("Numeric cols:", num_cols)
print("Categorical cols:", cat_cols)

# -------------------------
# 2) Pipeline (preprocessor + logistic)
# -------------------------
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
    ],
    remainder="drop",
)

clf = LogisticRegression(max_iter=2000, solver="saga", random_state=42)

pipe = Pipeline(steps=[("pre", preprocessor), ("clf", clf)])

# train/test split
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    X_raw, y, test_size=0.20, stratify=y, random_state=42
)

# quick grid (small)
param_grid = {
    "clf__C": [0.01, 0.1, 1],
    "clf__penalty": ["l2"],        # keep small for speed; saga supports elastic if needed
}
cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)
gs = GridSearchCV(pipe, param_grid, scoring="roc_auc", cv=cv, n_jobs=-1, verbose=1)
gs.fit(X_train_raw, y_train)

best = gs.best_estimator_
print("Best params:", gs.best_params_)
print("Best CV ROC-AUC:", gs.best_score_)

# evaluate on test
proba_test = best.predict_proba(X_test_raw)[:, 1]
print("Test ROC-AUC:", roc_auc_score(y_test, proba_test))
print(classification_report(y_test, (proba_test >= 0.5).astype(int)))

# -------------------------
# 3) Permutation importance (raw features)
# -------------------------
r = permutation_importance(best, X_test_raw, y_test, scoring="roc_auc", n_repeats=15, random_state=42)
pi = pd.DataFrame({"feature": X_test_raw.columns, "importance": r.importances_mean}).sort_values("importance", ascending=False)
print("\nTop permutation importances (raw features):\n", pi.head(15).to_string(index=False))

# plot topk
topk = pi.head(15).iloc[::-1]
plt.figure(figsize=(8, max(3, 0.25*len(topk))))
plt.barh(topk["feature"], topk["importance"])
plt.xlabel("Importance (Δ ROC-AUC)")
plt.title("Permutation Importance (Top Features)")
plt.tight_layout()
plt.show()

# -------------------------
# 4) Prepare SHAP on transformed space (Option 2)
# -------------------------
pre = best.named_steps["pre"]
clf_only = best.named_steps["clf"]

# transform train/test
X_train_trans = pre.transform(X_train_raw)
X_test_trans  = pre.transform(X_test_raw)

# expanded feature names after preprocessing (numeric + ohe)
ohe = pre.named_transformers_["cat"]
ohe_feats = ohe.get_feature_names_out(cat_cols).tolist() if len(cat_cols) else []
feat_names_transformed = num_cols + ohe_feats

# background sample for SHAP
bg_size = min(200, X_train_trans.shape[0])
rng = np.random.RandomState(42)
bg_idx = rng.choice(X_train_trans.shape[0], size=bg_size, replace=False)
background = X_train_trans[bg_idx]

# linear explainer
lin_explainer = shap.LinearExplainer(clf_only, background, feature_perturbation="interventional")

# compute SHAP values for test (not all if huge)
shap_test = lin_explainer(X_test_trans)   # Explanation object

# global: beeswarm (may be heavy)
print("Rendering SHAP beeswarm (global) — close plot to continue.")
shap.plots.beeswarm(shap_test, max_display=20)
plt.show()

# local example: waterfall for a chosen index
local_i = 5 if len(X_test_trans) > 5 else 0
print(f"Showing SHAP waterfall for test index {local_i}")
shap.plots.waterfall(shap_test[local_i], max_display=20)
plt.show()

# -------------------------
# 5) LIME on transformed space (local)
# -------------------------
lime_explainer = LimeTabularExplainer(
    training_data=X_train_trans,
    feature_names=feat_names_transformed,
    class_names=["No Default", "Default"],
    mode="classification",
    discretize_continuous=True
)

exp = lime_explainer.explain_instance(
    X_test_trans[local_i],
    predict_fn=clf_only.predict_proba,
    num_features=min(15, len(feat_names_transformed))
)
lime_html = "/content/lime_explanation.html"
exp.save_to_file(lime_html)
print("Saved LIME explanation to:", lime_html)

# -------------------------
# 6) Save artifacts
# -------------------------
joblib.dump(best, "/content/smartlend_pipe.joblib")
joblib.dump({"numerical": num_cols, "categorical": cat_cols}, "/content/smartlend_schema.joblib")
joblib.dump(feat_names_transformed, "/content/smartlend_featnames.joblib")
np.save("/content/smartlend_bg.npy", background)
print("Saved artifacts to /content: smartlend_pipe.joblib, smartlend_schema.joblib, smartlend_featnames.joblib, smartlend_bg.npy")

# -------------------------
# 7) Gradio UI (dynamic inputs matched to raw feature set)
# -------------------------
def _make_shap_bar_img(contrib, feat_names, top=15):
    order = np.argsort(np.abs(contrib))[-min(top, len(contrib)):]
    top_feats = [feat_names[j] for j in order]
    top_vals = contrib[order]
    fig, ax = plt.subplots(figsize=(8, max(3, 0.25*len(top_feats))))
    y = np.arange(len(top_feats))
    ax.barh(y, top_vals)
    ax.set_yticks(y)
    ax.set_yticklabels(top_feats)
    ax.set_xlabel("SHAP contribution (log-odds)")
    ax.set_title("Top feature contributions")
    plt.tight_layout()
    buf = io.BytesIO()
    plt.savefig(buf, format="png", bbox_inches="tight")
    plt.close(fig)
    buf.seek(0)
    return Image.open(buf)

# load artifacts for the app
pipe_app = joblib.load("/content/smartlend_pipe.joblib")
schema_app = joblib.load("/content/smartlend_schema.joblib")
feat_app = joblib.load("/content/smartlend_featnames.joblib")
bg_app = np.load("/content/smartlend_bg.npy")

pre_app = pipe_app.named_steps["pre"]
clf_app = pipe_app.named_steps["clf"]
shap_app_explainer = shap.LinearExplainer(clf_app, bg_app, feature_perturbation="interventional")

# build gradio inputs from schema (numerical first, then categorical)
gr_inputs = []
for c in schema_app["numerical"]:
    default_val = float(X_raw[c].median()) if c in X_raw.columns else 0.0
    gr_inputs.append(gr.Number(label=c, value=default_val))
for c in schema_app["categorical"]:
    choices = sorted(X_raw[c].dropna().unique().tolist()) if c in X_raw.columns else []
    default = choices[0] if choices else ""
    gr_inputs.append(gr.Dropdown(label=c, choices=choices, value=default))

def gr_predict_and_explain(*vals):
    # Reconstruct raw input row with exact columns order
    data = {}
    i = 0
    # numerical
    for c in schema_app["numerical"]:
        v = vals[i]
        try:
            data[c] = [float(v)]
        except:
            data[c] = [np.nan]
        i += 1
    # categorical
    for c in schema_app["categorical"]:
        v = vals[i]
        data[c] = [str(v)]
        i += 1

    X_in = pd.DataFrame(data)[X_raw.columns]  # ensure same column order
    # If missing columns (shouldn't), add NaNs
    for col in X_raw.columns:
        if col not in X_in.columns:
            X_in[col] = np.nan

    # prediction
    try:
        proba = pipe_app.predict_proba(X_in)[:, 1][0]
        pred_label = "DEFAULT" if proba >= 0.5 else "NON-DEFAULT"
        summary = f"Probability of default: {proba:.3f} → {pred_label}"
    except Exception as e:
        return f"Prediction error: {e}", None, f"<pre>{e}</pre>"

    # explain: transform then SHAP on transformed features
    try:
        Xin_trans = pre_app.transform(X_in)
        sv = shap_app_explainer(Xin_trans)
        contrib = sv.values[0]   # per-transformed-feature
        bar_img = _make_shap_bar_img(contrib, feat_app, top=15)
    except Exception as e:
        return summary, None, f"<pre>SHAP error: {e}</pre>"

    # LIME (heavy) — compute and return HTML
    try:
        lime_exp = LimeTabularExplainer(
            training_data=bg_app,
            feature_names=feat_app,
            class_names=["No Default", "Default"],
            mode="classification",
            discretize_continuous=True
        )
        exp = lime_exp.explain_instance(
            Xin_trans[0],
            predict_fn=clf_app.predict_proba,
            num_features=min(15, len(feat_app))
        )
        lime_html = exp.as_html()
    except Exception as e:
        lime_html = f"<pre>LIME error: {e}</pre>"

    return summary, bar_img, lime_html

# Build outputs (textbox, image, html)
demo = gr.Interface(
    fn=gr_predict_and_explain,
    inputs=gr_inputs,
    outputs=[
        gr.Textbox(label="Prediction"),
        gr.Image(type="pil", label="SHAP Top Contributions"),
        gr.HTML(label="LIME Explanation (local)")
    ],
    title="SmartLend — Loan Default Prediction & Explainability",
    description="Enter applicant features → get probability of default + SHAP contributions + LIME."
)

print("Launching Gradio (share=True). Click Submit after inputs are filled.")
demo.launch(share=True)